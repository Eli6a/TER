2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
1018
2036
/home/labicquette/.local/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning:
    There is an imbalance between your GPUs. You may want to exclude GPU 1 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))
2036
2036
2036
2036
2036
1018
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
2036
1018
/home/labicquette/.local/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning:
    There is an imbalance between your GPUs. You may want to exclude GPU 1 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))
2036
2036
2036
2036
2036
2036
1018
2032
{'input_ids': [0, 713, 16, 6, 10, 588, 131, 3645, 4, 1491, 269, 10661, 734, 50118, 1708, 24, 16, 50118, 1638, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
['<s>', 'This', 'Ġis', ',', 'Ġa', 'Ġreal', ';', 'Ġsentence', '.', 'ĠNot', 'Ġreally', 'ĠLegal', '...', 'Ċ', 'But', 'Ġit', 'Ġis', 'Ċ', 'ok', '.', '</s>']
{0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9', 10: '10'}
{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/labicquette/.local/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning:
    There is an imbalance between your GPUs. You may want to exclude GPU 1 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))
2024/04/25 16:47:01 ERROR mlflow.utils.async_logging.async_logging_queue: Run Id ece44bb2d426409c965a669aa8c92b64: Failed to log run data: Exception: Changing param values is not allowed. Param with key='logging_dir' was already logged with value='./models/bert-v0/runs/Apr25_16-45-07_DESKTOP-VOM7ARG' for run ID='ece44bb2d426409c965a669aa8c92b64'. Attempted logging new value './models/bert-v0/runs/Apr25_16-47-01_DESKTOP-VOM7ARG'.
/home/labicquette/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
/home/labicquette/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
/home/labicquette/.local/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning:
    There is an imbalance between your GPUs. You may want to exclude GPU 1 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.

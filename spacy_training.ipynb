{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#%cd source\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import spacy\n",
    "import os\n",
    "import pipeline\n",
    "from spacy.language import Language\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Règles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.custom_segmentation_component(doc)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajouter une règle de segmentation personnalisée\n",
    "def custom_segmentation(doc):\n",
    "    to_end_after_bracket = False\n",
    "    for i, token in enumerate(doc[:-1]):\n",
    "        \n",
    "        # si mot qui créer une phrase\n",
    "        if token.text == \"Id.\" or token.text == \"See\":\n",
    "            token.is_sent_start = True\n",
    "        \n",
    "        if token.text == \"Ibid.\" :\n",
    "            token.is_sent_start = True\n",
    "            if i < len(doc) - 2 :\n",
    "                doc[i + 1].is_sent_start = True\n",
    "\n",
    "        \n",
    "        # si point\n",
    "        if token.text == \".\":\n",
    "            token.is_sent_start = False               \n",
    "            \n",
    "            # Les parenthèses / crochets font partie de la phrase précédente\n",
    "            if doc[token.i + 1].text == \"[\" or doc[token.i + 1].text == \"(\" or doc[token.i + 2].text == \"[\" or doc[token.i + 2].text == \"(\":\n",
    "                doc[token.i + 1].is_sent_start = False\n",
    "                to_end_after_bracket = True\n",
    "            # Le point est suivi d'une minuscule   \n",
    "            elif re.match(r'^[a-z]', doc[i + 1].text):\n",
    "                doc[i + 1].is_sent_start = False\n",
    "            \n",
    "            # si ressemble au point d'un sigle\n",
    "            elif (i > 0 and re.match(r'^[A-Z][A-Za-z]{0,3}$|^[a-z]{1,2}$', doc[i-1].text) and i < len(doc) - 2):\n",
    "                doc[i + 1].is_sent_start = False\n",
    "            \n",
    "            # si dernier point des points de suspension    \n",
    "            elif (i > 2 and doc[i - 1].text == \".\" and doc[i - 2].text == \".\")  and i < len(doc) - 2:\n",
    "                doc[i + 1].is_sent_start = False\n",
    "                \n",
    "            continue\n",
    "        \n",
    "        # si fermer parenthèse ou crochet et to_end_after_bracket est vrai\n",
    "        if (token.text == \"]\" or token.text == \")\") and to_end_after_bracket:\n",
    "            # si suivi de parenthèses ou crochets, la phrase continue, sinon fin de phrase\n",
    "            if (doc[i + 1].text != \"(\" or doc[i + 1].text != \"[\"):\n",
    "                doc[i + 1].is_sent_start = True\n",
    "                to_end_after_bracket = False\n",
    "            # si suivi ponctuation, la phrase continue\n",
    "            elif (doc[i+1].text == \".\" or doc[i+1].text == \";\" or token.text == \":\"):\n",
    "                to_end_after_bracket = False\n",
    "            continue\n",
    "        \n",
    "        # si est entre parenthèse, continue la phrase    \n",
    "        if to_end_after_bracket :\n",
    "            doc[i + 1].is_sent_start = False\n",
    "            continue\n",
    "        \n",
    "        # si virgule, la virgule fait partie de la phrase précédente et pas de fin de phrase\n",
    "        if token.text == \",\" and i < len(doc) - 2 :\n",
    "            token.is_sent_start = False\n",
    "            doc[i + 1].is_sent_start = False\n",
    "            continue\n",
    "            \n",
    "        # si ponctuation, la ponctuation fait partie de la phrase précédente\n",
    "        if token.text == \";\" or token.text == \":\":\n",
    "            token.is_sent_start = False\n",
    "            # si suivie d'une minuscule, ne marque pas la fin de la phrase, sinon oui\n",
    "            if (i < len(doc) - 2 and re.match(r'^[a-z]', doc[i + 1].text)):\n",
    "                doc[i + 1].is_sent_start = False\n",
    "            elif (i < len(doc) - 2 and doc[i + 1].text == \"\\\"\"):\n",
    "                doc[i + 1].is_sent_start = True\n",
    "                if i < len(doc) - 3:\n",
    "                    doc[i + 2].is_sent_start = False\n",
    "            else :\n",
    "                doc[i + 1].is_sent_start = True\n",
    "            continue\n",
    "            \n",
    "    return doc\n",
    "\n",
    "# Ajouter la fonction de segmentation personnalisée au pipeline spaCy\n",
    "@Language.component(\"custom_segmentation\")\n",
    "def custom_segmentation_component(doc):\n",
    "    return custom_segmentation(doc)\n",
    "\n",
    "# Charger le modèle spaCy de base\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Insérer la segmentation personnalisée au début du pipeline\n",
    "nlp.add_pipe(\"custom_segmentation\", last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Règle qui ne marche pas : \n",
    "- Gestion des guillemets parce que s'il en manque un, tout est faussé parce que contrairement aux parenthèses/crochets, il y a une ambiguité sur le début ou la fin de citation\n",
    "- Gestion des points-virgules (parfois fin de phrases, parfois non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if token.text == \"\\\"\":\n",
    "#     # ouverture de citation\n",
    "#     if not in_quote :\n",
    "#         in_quote = True\n",
    "#         if i < len(doc) - 2 :\n",
    "#             doc[i + 1].is_sent_start = False\n",
    "        \n",
    "#     # fermeture de citation \n",
    "#     else :\n",
    "#         in_quote = False\n",
    "#         token.is_sent_start = False\n",
    "#         if i < len(doc) - 2 :\n",
    "#             doc[i + 1].is_sent_start = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluation sur train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_eval_train = []\n",
    "dir_path = \"../documents/train/*\"\n",
    "models = ['spacy', 'custom_spacy']\n",
    "tokenizers = ['spacy', 'spacy']\n",
    "for i in range(len(models)):\n",
    "    start = time.time()\n",
    "    r, p, f = pipeline.evaluation(dir_path, tokenizers[i], models[i])\n",
    "    res_eval_train += [[time.time()-start, r, p, f]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f8de9_row0_col0 {\n",
       "  background-color: red;\n",
       "  background-color: green;\n",
       "}\n",
       "#T_f8de9_row0_col1, #T_f8de9_row0_col2, #T_f8de9_row0_col3 {\n",
       "  background-color: red;\n",
       "}\n",
       "#T_f8de9_row1_col0 {\n",
       "  background-color: green;\n",
       "  background-color: red;\n",
       "}\n",
       "#T_f8de9_row1_col1, #T_f8de9_row1_col2, #T_f8de9_row1_col3 {\n",
       "  background-color: green;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f8de9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f8de9_level0_col0\" class=\"col_heading level0 col0\" >execution_time</th>\n",
       "      <th id=\"T_f8de9_level0_col1\" class=\"col_heading level0 col1\" >precision</th>\n",
       "      <th id=\"T_f8de9_level0_col2\" class=\"col_heading level0 col2\" >recall</th>\n",
       "      <th id=\"T_f8de9_level0_col3\" class=\"col_heading level0 col3\" >F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f8de9_level0_row0\" class=\"row_heading level0 row0\" >spacy</th>\n",
       "      <td id=\"T_f8de9_row0_col0\" class=\"data row0 col0\" >3.219471</td>\n",
       "      <td id=\"T_f8de9_row0_col1\" class=\"data row0 col1\" >0.830381</td>\n",
       "      <td id=\"T_f8de9_row0_col2\" class=\"data row0 col2\" >0.705240</td>\n",
       "      <td id=\"T_f8de9_row0_col3\" class=\"data row0 col3\" >0.761185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f8de9_level0_row1\" class=\"row_heading level0 row1\" >custom_spacy</th>\n",
       "      <td id=\"T_f8de9_row1_col0\" class=\"data row1 col0\" >18.135807</td>\n",
       "      <td id=\"T_f8de9_row1_col1\" class=\"data row1 col1\" >0.940357</td>\n",
       "      <td id=\"T_f8de9_row1_col2\" class=\"data row1 col2\" >0.866743</td>\n",
       "      <td id=\"T_f8de9_row1_col3\" class=\"data row1 col3\" >0.901046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fdc60a1c8b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "little_df = pd.DataFrame(res_eval_train,columns=[\"execution_time\",\"precision\", \"recall\", \"F1_score\"], index=['spacy', \"custom_spacy\"])\n",
    "little_df.style.highlight_max(color = 'green', axis = 0).highlight_max(color = 'red', axis = 0, subset = [\"execution_time\"]).highlight_min(color = 'red', axis = 0).highlight_min(color = 'green', axis = 0, subset = [\"execution_time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluation sur test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_eval_test = []\n",
    "dir_path = \"../documents/test/*\"\n",
    "models = ['spacy', 'custom_spacy']\n",
    "tokenizers = ['spacy', 'spacy']\n",
    "for i in range(len(models)):\n",
    "    start = time.time()\n",
    "    r, p, f = pipeline.evaluation(dir_path, tokenizers[i], models[i])\n",
    "    res_eval_test += [[time.time()-start, r, p, f]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_cb495_row0_col0 {\n",
       "  background-color: red;\n",
       "  background-color: green;\n",
       "}\n",
       "#T_cb495_row0_col1, #T_cb495_row0_col2, #T_cb495_row0_col3 {\n",
       "  background-color: red;\n",
       "}\n",
       "#T_cb495_row1_col0 {\n",
       "  background-color: green;\n",
       "  background-color: red;\n",
       "}\n",
       "#T_cb495_row1_col1, #T_cb495_row1_col2, #T_cb495_row1_col3 {\n",
       "  background-color: green;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_cb495\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cb495_level0_col0\" class=\"col_heading level0 col0\" >execution_time</th>\n",
       "      <th id=\"T_cb495_level0_col1\" class=\"col_heading level0 col1\" >precision</th>\n",
       "      <th id=\"T_cb495_level0_col2\" class=\"col_heading level0 col2\" >recall</th>\n",
       "      <th id=\"T_cb495_level0_col3\" class=\"col_heading level0 col3\" >F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cb495_level0_row0\" class=\"row_heading level0 row0\" >spacy</th>\n",
       "      <td id=\"T_cb495_row0_col0\" class=\"data row0 col0\" >0.633920</td>\n",
       "      <td id=\"T_cb495_row0_col1\" class=\"data row0 col1\" >0.809799</td>\n",
       "      <td id=\"T_cb495_row0_col2\" class=\"data row0 col2\" >0.770942</td>\n",
       "      <td id=\"T_cb495_row0_col3\" class=\"data row0 col3\" >0.787504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cb495_level0_row1\" class=\"row_heading level0 row1\" >custom_spacy</th>\n",
       "      <td id=\"T_cb495_row1_col0\" class=\"data row1 col0\" >4.051026</td>\n",
       "      <td id=\"T_cb495_row1_col1\" class=\"data row1 col1\" >0.874458</td>\n",
       "      <td id=\"T_cb495_row1_col2\" class=\"data row1 col2\" >0.928753</td>\n",
       "      <td id=\"T_cb495_row1_col3\" class=\"data row1 col3\" >0.899522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fdc5addae60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "little_df = pd.DataFrame(res_eval_test,columns=[\"execution_time\",\"precision\", \"recall\", \"F1_score\"], index=['spacy', \"custom_spacy\"])\n",
    "little_df.style.highlight_max(color = 'green', axis = 0).highlight_max(color = 'red', axis = 0, subset = [\"execution_time\"]).highlight_min(color = 'red', axis = 0).highlight_min(color = 'green', axis = 0, subset = [\"execution_time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test finetune spacy (abandonné)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"en_core_web_sm\"\n",
    "nlp = spacy.load(base_model)\n",
    "\n",
    "if \"sentencizer\" not in nlp.pipe_names:\n",
    "    sentencizer = nlp.add_pipe(\"sentencizer\")\n",
    "    \n",
    "# Préparation des données d'entraînement\n",
    "def process_text_files(directory):\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, filename), \"r\") as file:\n",
    "                text = file.read()\n",
    "                # Supprimer les sauts de ligne supplémentaires\n",
    "                text = text.strip()\n",
    "                # Séparer le texte en phrases\n",
    "                sentences = text.split(\"\\n\")\n",
    "                start_end_pos = []\n",
    "                start_pos = 0\n",
    "                for sentence in sentences:\n",
    "                    end_pos = start_pos + len(sentence)\n",
    "                    start_end_pos.append((start_pos, end_pos))\n",
    "                    # Ajouter la longueur d'un saut de ligne\n",
    "                    start_pos = end_pos + 1\n",
    "                data.append((text, {\"words\": sentences}))\n",
    "    return data\n",
    "\n",
    "# Définir le répertoire contenant vos fichiers .txt\n",
    "train_directory = \"../documents/train/\"\n",
    "\n",
    "# Charger les données d'entraînement\n",
    "train_data = process_text_files(train_directory)\n",
    "\n",
    "nlp.disable_pipes(\"tagger\", \"parser\")  # Désactiver le Tagger et le Parser par exemple\n",
    "\n",
    "# Entraînement du modèle\n",
    "# Utiliser vos données d'entraînement pour affiner le modèle\n",
    "for text, annotations in train_data:\n",
    "    # Créer un objet Example à partir du texte et des annotations\n",
    "    example = spacy.training.Example.from_dict(nlp.make_doc(text), annotations)\n",
    "    # Mettre à jour le modèle avec l'exemple\n",
    "    nlp.update([example], losses={})\n",
    "\n",
    "# Sauvegarder le modèle affiné\n",
    "nlp.to_disk(\"../models/fine_tuned_spacy_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"../models/fine_tuned_spacy_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence. [\n",
      "1] This is another sentence.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labicquette/.local/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp(\"This is a sentence. [1] This is another sentence.\")\n",
    "sentences = []\n",
    "for sentence in doc.sents:\n",
    "    #print(sentence.text)\n",
    "    sentences += [sentence.text]\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
